Text model LLM vs Multi model LLM:
Text model LLMs are primarily focused on processing textual data, while multimodal LLMs can process and understand multiple types of data like vedio , image, document and text.
Restriction in number of tokens, it will allow to prompt upto certain lenght of text.


Is there any advantage of using Multi model LLM for text processing:
The key benefits of multi-model systems include task specialization, increased efficiency, better accuracy, cost optimization, and the ability to combine models for multi-modal or multi-step tasks.
It will allow to prompt huge size text compare to text model LLM

Is Text model / Multi model / size (3B / 70B) decides the vector embedding size?


Can we do vector embedding using one LLM and generate response via different LLM?
Yes.
In a Retrieval-Augmented Generation (RAG) system, it is entirely feasible to use one LLM for vector embeddings (for retrieval purposes) and a different LLM for response generation. This is actually a common setup in many advanced RAG-based systems, where different models are optimized for specific tasksâ€”retrieval and generation.
Embedding models (often smaller or optimized for retrieval) can be run on less expensive hardware (like CPUs), while larger generative models can be run on GPUs only when needed for generation

Vector stores:
https://python.langchain.com/v0.1/docs/integrations/vectorstores/
FAISS, Pinecone, Milvus, or Weaviate - top vector stores
Couchbase - support vector storage and retrieval, but not effective in retrival, need to integrate it with specialized vector search systems such as FAISS, Pinecone, Milvus, or Weaviate
Redis - support vector storage and effective retrieval, but it ephemeral.
Postgres - support vector storage and effective retrieval with extention pg vector.
Amazon Aurora PostgreSQL-Compatible Edition and Amazon Relational Database Service (Amazon RDS) for PostgreSQL support the pgvector extension to store embeddings from machine learning (ML) models in your database and to perform efficient similarity searches.
Google Cloud offers relational databases AlloyDB for PostgreSQL and Cloud SQL for PostgreSQL that support building generative AI applications using the pgvector PostgreSQL extension, LangChain, and large language models (LLMs).
Azure Cosmos DB for PostgreSQL,
Note : All CSPs offer one vector DB for unstructured data and one for RDBMS. Postgres is the one available as the managed vecor database in all CSPs

Tools using LLAM 3 , speed comparision:
https://groq.com/

Chat with LLAMA 3 via online:
https://groq.com/

LLMA - transformer - token:
https://huggingface.co/docs/transformers/main/en/model_doc/llama
Online token counter - https://huggingface.co/spaces/Xanthius/llama-token-counter

What is max size of token (input/output) for LLM?
Most of basic model including llma support 4000 size of tokens for input
To increase the token size, LLM has to be tuned. Need more info on this.

Why RAG is appropriate solution for doamin use case than training / tuning?

LLMA installation guide:
https://www.youtube.com/watch?v=7r3eGE9ryJw

Full text search vs Semantic search vs nearest neighbor search:


Prompt local LLAMA via API:
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "What are Atoms?" }
  ],
  "stream": false
}'

Vector embedding using local llama:
curl http://localhost:11434/api/embeddings -d '{
  "model": "llama3.2",
  "prompt": "hello"
}'

